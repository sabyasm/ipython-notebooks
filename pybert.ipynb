{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 01_generate_features.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabyasm/ipython-notebooks/blob/master/pybert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xowUx222HMDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download & Install pytorch GPU version"
      ]
    },
    {
      "metadata": {
        "id": "eXLco-NbXDiF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fzKVpkH0tfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NVIDIA/apex.git\n",
        "#!python apex/setup.py install  \n",
        "#!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "boR7V6czJa9b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQ58aeWZ4nk6",
        "colab_type": "code",
        "outputId": "0c1b0fdb-797b-40bf-b335-69e10caf5814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/pytorch-pretrained-BERT.git"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pytorch-pretrained-BERT' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tmhKPdgr5hnR",
        "colab_type": "code",
        "outputId": "f5e04681-fe07-48d8-8972-472f743cddc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!curl -Lo dataset/msr_paraphrase_train.txt https://s3-us-west-2.amazonaws.com/manu00/msr_paraphrase_train.txt\n",
        "!curl -Lo dataset/msr_paraphrase_test.txt https://s3-us-west-2.amazonaws.com/manu00/msr_paraphrase_test.txt\n",
        "!curl -Lo dataset/Competition_Train_Data.txt https://s3-us-west-2.amazonaws.com/manu00/Competition_Train_Data.txt\n",
        "!curl -Lo dataset/Competition_Test_Data.txt https://s3-us-west-2.amazonaws.com/manu00/Competition_Test_Data.txt\n",
        "!curl -Lo download_glue_data.py https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/becd574dd938f045ea5bd3cb77d1d506541b5345/download_glue_data.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1002k  100 1002k    0     0  1731k      0 --:--:-- --:--:-- --:--:-- 1731k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  422k  100  422k    0     0   715k      0 --:--:-- --:--:-- --:--:--  714k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1102k  100 1102k    0     0  1663k      0 --:--:-- --:--:-- --:--:-- 1663k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  274k  100  274k    0     0   472k      0 --:--:-- --:--:-- --:--:--  472k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7775  100  7775    0     0  97187      0 --:--:-- --:--:-- --:--:-- 97187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R7Awo9b06mlT",
        "colab_type": "code",
        "outputId": "12fdd3bb-24f8-4e7e-9ef1-9053ca476681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr MRPC/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2812\n",
            "drwxr-xr-x 2 root root    4096 Dec 17 16:25 MRPC\n",
            "-rw-r--r-- 1 root root 1026746 Dec 17 17:14 msr_paraphrase_train.txt\n",
            "-rw-r--r-- 1 root root  432882 Dec 17 17:14 msr_paraphrase_test.txt\n",
            "-rw-r--r-- 1 root root 1129181 Dec 17 17:14 Competition_Train_Data.txt\n",
            "-rw-r--r-- 1 root root  280894 Dec 17 17:14 Competition_Test_Data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J-7l_Fgs-mDB",
        "colab_type": "code",
        "outputId": "f26ca824-7b18-466b-e210-e342a5b192ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "#!ls -1 dataset/MRPC \n",
        "#!wc -l MRPC/MRPC/dev_ids.tsv\n",
        "#!wc -l MRPC/MRPC/train.tsv\n",
        "#!wc -l MRPC/MRPC/test.tsv\n",
        "#!wc -l MRPC/MRPC/dev.tsv\n",
        "#!wc -l MRPC/MRPC/msr_paraphrase_train.txt\n",
        "#!wc -l MRPC/MRPC/msr_paraphrase_test.txt\n",
        "!cat dataset/MRPC/msr_paraphrase_train.txt |head -3\n",
        "!cat dataset/MRPC/train.tsv |head -3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
            "1\t702876\t702977\tAmrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\tReferring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
            "0\t2108705\t2108831\tYucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\tYucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n",
            "﻿Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
            "1\t702876\t702977\tAmrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\tReferring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
            "0\t2108705\t2108831\tYucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\tYucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TT28AXTNB7cZ",
        "colab_type": "code",
        "outputId": "08f0d2a2-25ad-4c44-db1d-357f53d1a250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr pytorch-pretrained-BERT/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 84\n",
            "-rw-r--r-- 1 root root 11358 Dec 17 15:57 LICENSE\n",
            "-rw-r--r-- 1 root root 36315 Dec 17 15:57 README.md\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 docker\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 bin\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 examples\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 notebooks\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 pytorch_pretrained_bert\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 tests\n",
            "-rw-r--r-- 1 root root  2634 Dec 17 15:57 setup.py\n",
            "drwxr-xr-x 2 root root  4096 Dec 17 15:57 samples\n",
            "-rw-r--r-- 1 root root   173 Dec 17 15:57 requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qw6xAXml-FV2",
        "colab_type": "code",
        "outputId": "9580abd8-ab4e-4f25-c10e-a45fca0f549a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!python download_glue_data.py --data_dir='dataset' --tasks='MRPC' --path_to_mrpc=''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing MRPC...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nskV2DpNB1af",
        "colab_type": "code",
        "outputId": "b8339957-5508-403c-b53d-97f6e25017ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4117
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!export GLUE_DIR=/content/MRPC/\n",
        "\n",
        "!python pytorch-pretrained-BERT/examples/run_classifier_orig.py \\\n",
        "  --task_name MRPC \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --data_dir /content/dataset/MRPC/ \\\n",
        "  --bert_model bert-base-uncased \\\n",
        "  --max_seq_length 128 \\\n",
        "  --train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --output_dir /tmp/mrpc_output/"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "12/17/2018 18:56:48 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "12/17/2018 18:56:49 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/17/2018 18:56:49 - INFO - __main__ -   LOOKING AT /content/dataset/MRPC/train.tsv\n",
            "12/17/2018 18:56:49 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "12/17/2018 18:56:49 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp43f303zq\n",
            "12/17/2018 18:56:54 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/17/2018 18:56:58 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/17/2018 18:56:58 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   guid: train-1\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   label: 1 (id = 1)\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   guid: train-2\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   tokens: [CLS] yu ##ca ##ip ##a owned dominic ##k ' s before selling the chain to safe ##way in 1998 for $ 2 . 5 billion . [SEP] yu ##ca ##ip ##a bought dominic ##k ' s in 1995 for $ 69 ##3 million and sold it to safe ##way for $ 1 . 8 billion in 1998 . [SEP]\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   label: 0 (id = 0)\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   guid: train-3\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   tokens: [CLS] they had published an advertisement on the internet on june 10 , offering the cargo for sale , he added . [SEP] on june 10 , the ship ' s owners had published an advertisement on the internet , offering the explosives for sale . [SEP]\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   label: 1 (id = 1)\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   guid: train-4\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   tokens: [CLS] around 03 ##35 gm ##t , tab shares were up 19 cents , or 4 . 4 % , at a $ 4 . 56 , having earlier set a record high of a $ 4 . 57 . [SEP] tab shares jumped 20 cents , or 4 . 6 % , to set a record closing high at a $ 4 . 57 . [SEP]\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   label: 0 (id = 0)\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   guid: train-5\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   tokens: [CLS] the stock rose $ 2 . 11 , or about 11 percent , to close friday at $ 21 . 51 on the new york stock exchange . [SEP] pg & e corp . shares jumped $ 1 . 63 or 8 percent to $ 21 . 03 on the new york stock exchange on friday . [SEP]\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 18:57:01 - INFO - __main__ -   label: 1 (id = 1)\n",
            "12/17/2018 18:57:05 - INFO - __main__ -   ***** Running training *****\n",
            "12/17/2018 18:57:05 - INFO - __main__ -     Num examples = 3668\n",
            "12/17/2018 18:57:05 - INFO - __main__ -     Batch size = 32\n",
            "12/17/2018 18:57:05 - INFO - __main__ -     Num steps = 114\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:01<03:19,  1.75s/it]\u001b[A\n",
            "Iteration:   2% 2/115 [00:03<03:15,  1.73s/it]\u001b[A\n",
            "Iteration:   3% 3/115 [00:05<03:12,  1.72s/it]\u001b[A\n",
            "Iteration:   3% 4/115 [00:06<03:10,  1.72s/it]\u001b[A\n",
            "Iteration:   4% 5/115 [00:08<03:08,  1.72s/it]\u001b[A\n",
            "Iteration:   5% 6/115 [00:10<03:06,  1.71s/it]\u001b[A\n",
            "Iteration:   6% 7/115 [00:11<03:04,  1.71s/it]\u001b[A\n",
            "Iteration:   7% 8/115 [00:13<03:02,  1.71s/it]\u001b[A\n",
            "Iteration:   8% 9/115 [00:15<03:01,  1.71s/it]\u001b[A\n",
            "Iteration:   9% 10/115 [00:17<03:00,  1.72s/it]\u001b[A\n",
            "Iteration:  10% 11/115 [00:18<02:58,  1.71s/it]\u001b[A\n",
            "Iteration:  10% 12/115 [00:20<02:56,  1.71s/it]\u001b[A\n",
            "Iteration:  11% 13/115 [00:22<02:54,  1.71s/it]\u001b[A\n",
            "Iteration:  12% 14/115 [00:23<02:52,  1.71s/it]\u001b[A\n",
            "Iteration:  13% 15/115 [00:25<02:51,  1.71s/it]\u001b[A\n",
            "Iteration:  14% 16/115 [00:27<02:49,  1.71s/it]\u001b[A\n",
            "Iteration:  15% 17/115 [00:29<02:47,  1.71s/it]\u001b[A\n",
            "Iteration:  16% 18/115 [00:30<02:45,  1.71s/it]\u001b[A\n",
            "Iteration:  17% 19/115 [00:32<02:43,  1.71s/it]\u001b[A\n",
            "Iteration:  17% 20/115 [00:34<02:42,  1.71s/it]\u001b[A\n",
            "Iteration:  18% 21/115 [00:35<02:40,  1.71s/it]\u001b[A\n",
            "Iteration:  19% 22/115 [00:37<02:38,  1.71s/it]\u001b[A\n",
            "Iteration:  20% 23/115 [00:39<02:36,  1.71s/it]\u001b[A\n",
            "Iteration:  21% 24/115 [00:41<02:34,  1.70s/it]\u001b[A\n",
            "Iteration:  22% 25/115 [00:42<02:33,  1.70s/it]\u001b[A\n",
            "Iteration:  23% 26/115 [00:44<02:31,  1.70s/it]\u001b[A\n",
            "Iteration:  23% 27/115 [00:46<02:29,  1.70s/it]\u001b[A\n",
            "Iteration:  24% 28/115 [00:47<02:27,  1.70s/it]\u001b[A\n",
            "Iteration:  25% 29/115 [00:49<02:26,  1.70s/it]\u001b[A\n",
            "Iteration:  26% 30/115 [00:51<02:24,  1.70s/it]\u001b[A\n",
            "Iteration:  27% 31/115 [00:52<02:23,  1.70s/it]\u001b[A\n",
            "Iteration:  28% 32/115 [00:54<02:21,  1.71s/it]\u001b[A\n",
            "Iteration:  29% 33/115 [00:56<02:20,  1.71s/it]\u001b[A\n",
            "Iteration:  30% 34/115 [00:58<02:18,  1.71s/it]\u001b[A\n",
            "Iteration:  30% 35/115 [00:59<02:16,  1.71s/it]\u001b[A\n",
            "Iteration:  31% 36/115 [01:01<02:14,  1.70s/it]\u001b[A\n",
            "Iteration:  32% 37/115 [01:03<02:13,  1.71s/it]\u001b[A\n",
            "Iteration:  33% 38/115 [01:04<02:11,  1.71s/it]\u001b[A\n",
            "Iteration:  34% 39/115 [01:06<02:09,  1.70s/it]\u001b[A\n",
            "Iteration:  35% 40/115 [01:08<02:07,  1.70s/it]\u001b[A\n",
            "Iteration:  36% 41/115 [01:09<02:05,  1.69s/it]\u001b[A\n",
            "Iteration:  37% 42/115 [01:11<02:03,  1.69s/it]\u001b[A\n",
            "Iteration:  37% 43/115 [01:13<02:01,  1.69s/it]\u001b[A\n",
            "Iteration:  38% 44/115 [01:15<02:00,  1.70s/it]\u001b[A\n",
            "Iteration:  39% 45/115 [01:16<01:58,  1.70s/it]\u001b[A\n",
            "Iteration:  40% 46/115 [01:18<01:57,  1.70s/it]\u001b[A\n",
            "Iteration:  41% 47/115 [01:20<01:55,  1.70s/it]\u001b[A\n",
            "Iteration:  42% 48/115 [01:21<01:53,  1.70s/it]\u001b[A\n",
            "Iteration:  43% 49/115 [01:23<01:51,  1.69s/it]\u001b[A\n",
            "Iteration:  43% 50/115 [01:25<01:50,  1.69s/it]\u001b[A\n",
            "Iteration:  44% 51/115 [01:26<01:48,  1.69s/it]\u001b[A\n",
            "Iteration:  45% 52/115 [01:28<01:46,  1.69s/it]\u001b[A\n",
            "Iteration:  46% 53/115 [01:30<01:45,  1.70s/it]\u001b[A\n",
            "Iteration:  47% 54/115 [01:32<01:43,  1.70s/it]\u001b[A\n",
            "Iteration:  48% 55/115 [01:33<01:42,  1.70s/it]\u001b[A\n",
            "Iteration:  49% 56/115 [01:35<01:40,  1.70s/it]\u001b[A\n",
            "Iteration:  50% 57/115 [01:37<01:38,  1.70s/it]\u001b[A\n",
            "Iteration:  50% 58/115 [01:38<01:36,  1.69s/it]\u001b[A\n",
            "Iteration:  51% 59/115 [01:40<01:34,  1.69s/it]\u001b[A\n",
            "Iteration:  52% 60/115 [01:42<01:33,  1.69s/it]\u001b[A\n",
            "Iteration:  53% 61/115 [01:43<01:31,  1.69s/it]\u001b[A\n",
            "Iteration:  54% 62/115 [01:45<01:29,  1.69s/it]\u001b[A\n",
            "Iteration:  55% 63/115 [01:47<01:28,  1.69s/it]\u001b[A\n",
            "Iteration:  56% 64/115 [01:48<01:26,  1.69s/it]\u001b[A\n",
            "Iteration:  57% 65/115 [01:50<01:24,  1.69s/it]\u001b[A\n",
            "Iteration:  57% 66/115 [01:52<01:23,  1.69s/it]\u001b[A\n",
            "Iteration:  58% 67/115 [01:54<01:21,  1.69s/it]\u001b[A\n",
            "Iteration:  59% 68/115 [01:55<01:19,  1.70s/it]\u001b[A\n",
            "Iteration:  60% 69/115 [01:57<01:18,  1.70s/it]\u001b[A\n",
            "Iteration:  61% 70/115 [01:59<01:16,  1.69s/it]\u001b[A\n",
            "Iteration:  62% 71/115 [02:00<01:14,  1.69s/it]\u001b[A\n",
            "Iteration:  63% 72/115 [02:02<01:12,  1.69s/it]\u001b[A\n",
            "Iteration:  63% 73/115 [02:04<01:11,  1.69s/it]\u001b[A\n",
            "Iteration:  64% 74/115 [02:05<01:09,  1.69s/it]\u001b[A\n",
            "Iteration:  65% 75/115 [02:07<01:07,  1.69s/it]\u001b[A\n",
            "Iteration:  66% 76/115 [02:09<01:06,  1.69s/it]\u001b[A\n",
            "Iteration:  67% 77/115 [02:10<01:04,  1.69s/it]\u001b[A\n",
            "Iteration:  68% 78/115 [02:12<01:02,  1.69s/it]\u001b[A\n",
            "Iteration:  69% 79/115 [02:14<01:00,  1.69s/it]\u001b[A\n",
            "Iteration:  70% 80/115 [02:16<00:59,  1.69s/it]\u001b[A\n",
            "Iteration:  70% 81/115 [02:17<00:57,  1.69s/it]\u001b[A\n",
            "Iteration:  71% 82/115 [02:19<00:55,  1.70s/it]\u001b[A\n",
            "Iteration:  72% 83/115 [02:21<00:54,  1.69s/it]\u001b[A\n",
            "Iteration:  73% 84/115 [02:22<00:52,  1.69s/it]\u001b[A\n",
            "Iteration:  74% 85/115 [02:24<00:50,  1.69s/it]\u001b[A\n",
            "Iteration:  75% 86/115 [02:26<00:48,  1.69s/it]\u001b[A\n",
            "Iteration:  76% 87/115 [02:27<00:47,  1.69s/it]\u001b[A\n",
            "Iteration:  77% 88/115 [02:29<00:45,  1.69s/it]\u001b[A\n",
            "Iteration:  77% 89/115 [02:31<00:44,  1.70s/it]\u001b[A\n",
            "Iteration:  78% 90/115 [02:32<00:42,  1.70s/it]\u001b[A\n",
            "Iteration:  79% 91/115 [02:34<00:40,  1.69s/it]\u001b[A\n",
            "Iteration:  80% 92/115 [02:36<00:38,  1.69s/it]\u001b[A\n",
            "Iteration:  81% 93/115 [02:38<00:37,  1.69s/it]\u001b[A\n",
            "Iteration:  82% 94/115 [02:39<00:35,  1.69s/it]\u001b[A\n",
            "Iteration:  83% 95/115 [02:41<00:33,  1.69s/it]\u001b[A\n",
            "Iteration:  83% 96/115 [02:43<00:32,  1.69s/it]\u001b[A\n",
            "Iteration:  84% 97/115 [02:44<00:30,  1.70s/it]\u001b[A\n",
            "Iteration:  85% 98/115 [02:46<00:28,  1.70s/it]\u001b[A\n",
            "Iteration:  86% 99/115 [02:48<00:27,  1.70s/it]\u001b[A\n",
            "Iteration:  87% 100/115 [02:49<00:25,  1.69s/it]\u001b[A\n",
            "Iteration:  88% 101/115 [02:51<00:23,  1.69s/it]\u001b[A\n",
            "Iteration:  89% 102/115 [02:53<00:22,  1.69s/it]\u001b[A\n",
            "Iteration:  90% 103/115 [02:54<00:20,  1.69s/it]\u001b[A\n",
            "Iteration:  90% 104/115 [02:56<00:18,  1.70s/it]\u001b[A\n",
            "Iteration:  91% 105/115 [02:58<00:16,  1.70s/it]\u001b[A\n",
            "Iteration:  92% 106/115 [03:00<00:15,  1.70s/it]\u001b[A\n",
            "Iteration:  93% 107/115 [03:01<00:13,  1.70s/it]\u001b[A\n",
            "Iteration:  94% 108/115 [03:03<00:11,  1.70s/it]\u001b[A\n",
            "Iteration:  95% 109/115 [03:05<00:10,  1.70s/it]\u001b[A\n",
            "Iteration:  96% 110/115 [03:06<00:08,  1.69s/it]\u001b[A\n",
            "Iteration:  97% 111/115 [03:08<00:06,  1.69s/it]\u001b[A\n",
            "Iteration:  97% 112/115 [03:10<00:05,  1.69s/it]\u001b[A\n",
            "Iteration:  98% 113/115 [03:11<00:03,  1.69s/it]\u001b[A\n",
            "Iteration:  99% 114/115 [03:13<00:01,  1.69s/it]\u001b[A\n",
            "Iteration: 100% 115/115 [03:14<00:00,  1.53s/it]\u001b[A\n",
            "Epoch: 100% 1/1 [03:14<00:00, 194.78s/it]\n",
            "12/17/2018 19:00:22 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "12/17/2018 19:00:22 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpfe1z6ayf\n",
            "12/17/2018 19:00:28 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   guid: dev-1\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] \" the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   label: 1 (id = 1)\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   guid: dev-2\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   tokens: [CLS] magna ##relli said ra ##cic ##ot hated the iraqi regime and looked forward to using his long years of training in the war . [SEP] his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war . [SEP]\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   label: 0 (id = 0)\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   guid: dev-3\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   tokens: [CLS] the dollar was at 116 . 92 yen against the yen , flat on the session , and at 1 . 289 ##1 against the swiss fran ##c , also flat . [SEP] the dollar was at 116 . 78 yen jp ##y = , virtually flat on the session , and at 1 . 287 ##1 against the swiss fran ##c ch ##f = , down 0 . 1 percent . [SEP]\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   label: 0 (id = 0)\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   guid: dev-4\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   tokens: [CLS] the afl - ci ##o is waiting until october to decide if it will end ##ors ##e a candidate . [SEP] the afl - ci ##o announced wednesday that it will decide in october whether to end ##ors ##e a candidate before the primaries . [SEP]\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   label: 1 (id = 1)\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   *** Example ***\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   guid: dev-5\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   tokens: [CLS] no dates have been set for the civil or the criminal trial . [SEP] no dates have been set for the criminal or civil cases , but shan ##ley has pleaded not guilty . [SEP]\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/17/2018 19:00:31 - INFO - __main__ -   label: 0 (id = 0)\n",
            "12/17/2018 19:00:32 - INFO - __main__ -   ***** Running evaluation *****\n",
            "12/17/2018 19:00:32 - INFO - __main__ -     Num examples = 408\n",
            "12/17/2018 19:00:32 - INFO - __main__ -     Batch size = 8\n",
            "12/17/2018 19:00:46 - INFO - __main__ -   ***** Eval results *****\n",
            "12/17/2018 19:00:46 - INFO - __main__ -     eval_accuracy = 0.6838235294117647\n",
            "12/17/2018 19:00:46 - INFO - __main__ -     eval_loss = 0.5770487832088097\n",
            "12/17/2018 19:00:46 - INFO - __main__ -     global_step = 115\n",
            "12/17/2018 19:00:46 - INFO - __main__ -     loss = 0.6566213550774948\n",
            "CPU times: user 1.57 s, sys: 212 ms, total: 1.78 s\n",
            "Wall time: 4min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MHd1s9T5PUQI",
        "colab_type": "code",
        "outputId": "1de5dc6b-a13e-4f4b-aa30-e462fe5ea9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr /tmp/mrpc_output/\n",
        "!cat /tmp/mrpc_output//eval_results.txt\n",
        "!rm -rf /tmp/mrpc_output/"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427724\n",
            "-rw-r--r-- 1 root root 437981369 Dec 17 18:45 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root       111 Dec 17 18:46 eval_results.txt\n",
            "eval_accuracy = 0.8382352941176471\n",
            "eval_loss = 0.5102683555553941\n",
            "global_step = 575\n",
            "loss = 0.09123394388867462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LLKQDtIsM_gL",
        "colab_type": "code",
        "outputId": "5044aa1b-5353-402b-f4bc-0e77cf9a4256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "#!echo \"import apex\"  > pytorch-pretrained-BERT/examples/ap.py\n",
        "##!cat pytorch-pretrained-BERT/examples/ap.py pytorch-pretrained-BERT/examples/run_classifier.py > pytorch-pretrained-BERT/examples/run_classifier_apex.py\n",
        "#!cat pytorch-pretrained-BERT/examples/run_classifier.py |head -23\n",
        "#!sed -i '21iimport apex' pytorch-pretrained-BERT/examples/run_classifier.py\n",
        "#logits \n",
        "#!cp pytorch-pretrained-BERT/examples/run_classifier.py pytorch-pretrained-BERT/examples/run_classifier_orig.py\n",
        "#!cp pytorch-pretrained-BERT/examples/run_classifier_orig.py pytorch-pretrained-BERT/examples/run_classifier_01.py\n",
        "#!sed -i '605i\\\\tprint(str(logits))' pytorch-pretrained-BERT/examples/run_classifier_01.py\n",
        "#sed -i '605i\\\\    print(str(logits))' pytorch-pretrained-BERT/examples/run_classifier_01.py\n",
        "\n",
        "!sed -i '605d' pytorch-pretrained-BERT/examples/run_classifier_01.py\n",
        "!cat pytorch-pretrained-BERT/examples/run_classifier_01.py |tail -20"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            nb_eval_examples += input_ids.size(0)\n",
            "            nb_eval_steps += 1\n",
            "\n",
            "        eval_loss = eval_loss / nb_eval_steps\n",
            "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
            "\n",
            "        result = {'eval_loss': eval_loss,\n",
            "                  'eval_accuracy': eval_accuracy,\n",
            "                  'global_step': global_step,\n",
            "                  'loss': tr_loss/nb_tr_steps}\n",
            "\n",
            "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
            "        with open(output_eval_file, \"w\") as writer:\n",
            "            logger.info(\"***** Eval results *****\")\n",
            "            for key in sorted(result.keys()):\n",
            "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
            "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-N_b81-MpEd",
        "colab_type": "code",
        "outputId": "818ae428-32c6-4343-df2c-f94bb369c6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "#!rm -rf /tmp/mrpc_output/\n",
        "expand"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-3e064e984b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexpand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'expand' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TxD512-kHVeA",
        "colab_type": "code",
        "outputId": "8bfb3567-be53-432f-fe44-c2552de3c330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "#!ls -ltr /tmp/mrpc_output/\n",
        "!cat /tmp/mrpc_output/eval_results.txt |head -10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval_accuracy = 0.8357843137254902\n",
            "eval_loss = 0.3809997632223017\n",
            "global_step = 345\n",
            "loss = 0.23087687298007634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xsG0OtFW3gQv",
        "colab_type": "code",
        "outputId": "80d71579-2101-4766-c0ca-6383ce8fe49b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from nltk.tokenize import word_tokenize\n",
        "from numpy.random import RandomState\n",
        "import pickle\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.cross_validation import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "EolKlbnADZFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download RAW test and train files (I've loaded them in aws )"
      ]
    },
    {
      "metadata": {
        "id": "gc9xN6napswv",
        "colab_type": "code",
        "outputId": "becdf738-00ad-4fc9-96f1-0c6d173054d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!curl -Lo dataset/Competition_Train_Data.txt https://s3-us-west-2.amazonaws.com/manu00/Competition_Train_Data.txt\n",
        "!curl -Lo dataset/Competition_Test_Data.txt https://s3-us-west-2.amazonaws.com/manu00/Competition_Test_Data.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1102k  100 1102k    0     0   670k      0  0:00:01  0:00:01 --:--:--  670k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  274k  100  274k    0     0   278k      0 --:--:-- --:--:-- --:--:--  278k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dy09WJIcDvsg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Infersent with GloVe Embedding (let's call it v1)"
      ]
    },
    {
      "metadata": {
        "id": "eWGADKc5cTXG",
        "colab_type": "code",
        "outputId": "03e616d5-81c9-4d3d-cc4c-74e523f7b774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!curl -Lo models.py https://raw.githubusercontent.com/facebookresearch/InferSent/master/models.py\n",
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent1.pkl https://s3.amazonaws.com/senteval/infersent/infersent1.pkl\n",
        "!curl -Lo encoder/infersent2.pkl https://s3.amazonaws.com/senteval/infersent/infersent2.pkl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 29737  100 29737    0     0  85697      0 --:--:-- --:--:-- --:--:-- 85697\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  10.9M      0  0:00:13  0:00:13 --:--:-- 12.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  13.0M      0  0:00:11  0:00:11 --:--:-- 16.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mog-vBalEFLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2GB GloVe embedding Incoming"
      ]
    },
    {
      "metadata": {
        "id": "-rWcJKekYt7W",
        "colab_type": "code",
        "outputId": "3762bdd9-98ff-4d04-a54d-b16de94bbdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mkdir dataset/GloVe\n",
        "!curl -Lo dataset/GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip dataset/GloVe/glove.840B.300d.zip -d dataset/GloVe/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  5915k      0  0:05:59  0:05:59 --:--:-- 5990k\n",
            "Archive:  dataset/GloVe/glove.840B.300d.zip\n",
            "  inflating: dataset/GloVe/glove.840B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uluXRXwqYOel",
        "colab_type": "code",
        "outputId": "476fde77-6fac-4f9c-c971-360b06eef206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from models import InferSent\n",
        "V = 1\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "infersent = InferSent(params_model)\n",
        "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "# Keep it on CPU or put it on GPU\n",
        "use_cuda = True\n",
        "infersent = infersent.cuda() if use_cuda else infersent\n",
        "\n",
        "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
        "model_version = V\n",
        "W2V_PATH = 'dataset/GloVe/glove.840B.300d.txt' if model_version == 1 else 'dataset/fastText/crawl-300d-2M.vec'\n",
        "infersent.set_w2v_path(W2V_PATH)\n",
        "infersent.build_vocab_k_words(K=500000) #load 50k most frequent word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I9ZVd3bJ8ZDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ** FastText Common Crawl - 1.5G Embedding - let's call Infersent V2**"
      ]
    },
    {
      "metadata": {
        "id": "9GTPRSvu8dy4",
        "colab_type": "code",
        "outputId": "601827a8-970a-4676-f7b8-adda51f22f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mkdir dataset/fastText\n",
        "!curl -Lo dataset/fastText/crawl-300d-2M.vec.zip https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
        "!unzip dataset/fastText/crawl-300d-2M.vec.zip -d dataset/fastText/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1453M  100 1453M    0     0  16.3M      0  0:01:28  0:01:28 --:--:-- 21.7M\n",
            "Archive:  dataset/fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: dataset/fastText/crawl-300d-2M.vec  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1xBxc38y9uEF",
        "colab_type": "code",
        "outputId": "0eb368f4-2439-42e3-a12a-6e64e83b3b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from models import InferSent\n",
        "V = 2\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "infersent2 = InferSent(params_model)\n",
        "infersent2.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "# Keep it on CPU or put it on GPU\n",
        "use_cuda = True\n",
        "infersent2 = infersent2.cuda() if use_cuda else infersent2\n",
        "\n",
        "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
        "model_version = V\n",
        "W2V_PATH = 'dataset/GloVe/glove.840B.300d.txt' if model_version == 1 else 'dataset/fastText/crawl-300d-2M.vec'\n",
        "infersent2.set_w2v_path(W2V_PATH)\n",
        "infersent2.build_vocab_k_words(K=500000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pzBD-q--E-sZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Let's try Google's Universal Sentense Encoder (Transfer) - [link](https://tfhub.dev/google/universal-sentence-encoder-large/3)**"
      ]
    },
    {
      "metadata": {
        "id": "l3_BX0x4tROv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **USE V3 model load**"
      ]
    },
    {
      "metadata": {
        "id": "nITkwa9Fta5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BChezKzBtxjT",
        "colab_type": "code",
        "outputId": "e6d3cee7-1539-424b-c666-02f086e2f0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "embed = hub.Module(module_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kri3LFvBX5xf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Download common crawl vector in Spacy specific format (1.5G - again)**"
      ]
    },
    {
      "metadata": {
        "id": "uBZNWLEcXfE9",
        "colab_type": "code",
        "outputId": "f539be2d-2e3b-4441-92ec-60efb6f312c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
        "!python -m spacy init-model en /tmp/fasttext-vectors --vectors-loc crawl-300d-2M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-06 18:51:59--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.20.97\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.20.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  21.9MB/s    in 71s     \n",
            "\n",
            "2018-11-06 18:53:11 (20.5 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n",
            "Reading vectors from crawl-300d-2M.vec.zip\n",
            "Open loc\n",
            "tcmalloc: large alloc 2400002048 bytes == 0x33ee000 @  0x7f6a9a5d5001 0x7f6a98349b85 0x7f6a983acb43 0x7f6a983aea86 0x7f6a98446868 0x511b75 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x586ddd 0x59cf9e 0x4f6903 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x4ffa2a 0x5117df 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338\n",
            "1999995it [03:41, 9034.14it/s]\n",
            "Creating model...\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "\u001b[93m    Sucessfully compiled vocab\u001b[0m\n",
            "    1999715 entries, 1999995 vectors\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zNwPEqF_ZjPJ",
        "colab_type": "code",
        "outputId": "a3d69d37-b60a-49cd-f3eb-7f5546a95c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "#Code for obtaining sentence vector from spacy module\n",
        "!python -m spacy download en\n",
        "import spacy\n",
        "nlp_ft = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6qFv7VYBaNPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_spacy_cc_similarity():\n",
        "  i = 0\n",
        "  trainfts = []\n",
        "  testfts =[]\n",
        "  while i < len(traintext[0]):\n",
        "    doc1 = nlp_ft(traintext[0][i])\n",
        "    doc2 = nlp_ft(traintext[1][i])\n",
        "    trainfts.append(doc2.similarity(doc1))\n",
        "    i = i+1\n",
        "  print(\"done\")\n",
        "  i = 0\n",
        "  while i < len(testtext[0]):\n",
        "    doc1 = nlp_ft(testtext[0][i])\n",
        "    doc2 = nlp_ft(testtext[1][i])\n",
        "    testfts.append(doc2.similarity(doc1))\n",
        "    i = i+1\n",
        "    #print(\" \"+str(i))\n",
        "  \n",
        "  return trainfts, testfts\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MGHNTxVt0Lp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Basic text features like count of common word etc. **"
      ]
    },
    {
      "metadata": {
        "id": "kq-nmVFJhTJj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uODJSdOmiBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feats(A, B):\n",
        "    \"\"\"\n",
        "    Compute additional features (similar to Socher et al.)\n",
        "    These alone should give the same result from their paper (~73.2 Acc)\n",
        "    \"\"\"\n",
        "    tA = [t.split() for t in A]\n",
        "    tB = [t.split() for t in B]\n",
        "    \n",
        "    nA = [[w for w in t if is_number(w)] for t in tA]\n",
        "    nB = [[w for w in t if is_number(w)] for t in tB]\n",
        "\n",
        "    features = np.zeros((len(A), 6))\n",
        "\n",
        "    # n1\n",
        "    for i in range(len(A)):\n",
        "        if set(nA[i]) == set(nB[i]):\n",
        "            features[i,0] = 1.\n",
        "\n",
        "    # n2\n",
        "    for i in range(len(A)):\n",
        "        if set(nA[i]) == set(nB[i]) and len(nA[i]) > 0:\n",
        "            features[i,1] = 1.\n",
        "\n",
        "    # n3\n",
        "    for i in range(len(A)):\n",
        "        if set(nA[i]) <= set(nB[i]) or set(nB[i]) <= set(nA[i]): \n",
        "            features[i,2] = 1.\n",
        "\n",
        "    # n4\n",
        "    for i in range(len(A)):\n",
        "        features[i,3] = 1.0 * len(set(tA[i]) & set(tB[i])) / len(set(tA[i]))\n",
        "\n",
        "    # n5\n",
        "    for i in range(len(A)):\n",
        "        features[i,4] = 1.0 * len(set(tA[i]) & set(tB[i])) / len(set(tB[i]))\n",
        "\n",
        "    # n6\n",
        "    for i in range(len(A)):\n",
        "        features[i,5] = 0.5 * ((1.0*len(tA[i]) / len(tB[i])) + (1.0*len(tB[i]) / len(tA[i])))\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbBQO85ikBMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_sentense_fts(modelname, loc='./dataset/'):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    print ('Preparing data...')\n",
        "    traintext, testtext, labels, uid = load_data(loc)\n",
        "\n",
        "    # Reduce logging output.\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    train_features = []\n",
        "    test_features = []\n",
        "    \n",
        "    if modelname == \"infersent\":\n",
        "        print ('Computing ' + modelname + 'embedding for train data...')\n",
        "\n",
        "        trainA = infersent.encode(traintext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        trainB = infersent.encode(traintext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "        print ('Computing ' + modelname + 'embedding for test data...')\n",
        "        \n",
        "        testA = infersent.encode(testtext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        testB = infersent.encode(testtext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "    if modelname == \"infersent2\":\n",
        "        print ('Computing ' + modelname + 'embedding for train data...')\n",
        "\n",
        "        trainA = infersent2.encode(traintext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        trainB = infersent2.encode(traintext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "        print ('Computing ' + modelname + 'embedding for test data...')\n",
        "        \n",
        "        testA = infersent2.encode(testtext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        testB = infersent2.encode(testtext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "    if modelname ==\"use_v3\":\n",
        "        \n",
        "        print ('Computing training USE encoding...')\n",
        "        with tf.Session() as session:\n",
        "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "            trainA = session.run(embed(traintext[0]))\n",
        "            trainB = session.run(embed(traintext[1]))\n",
        "            testA = session.run(embed(testtext[0]))\n",
        "            testB = session.run(embed(testtext[1]))\n",
        "    \n",
        "    train_features = np.c_[np.abs(trainA - trainB), trainA * trainB]\n",
        "    test_features = np.c_[np.abs(testA - testB), testA * testB]\n",
        "        \n",
        "    \n",
        "    return train_features, test_features\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Ukjst-Oj8_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(loc='./dataset/'):\n",
        "    \"\"\"\n",
        "    Load test and train dataset\n",
        "    \"\"\"\n",
        "    trainloc = os.path.join(loc, 'Competition_Train_Data.txt')\n",
        "    testloc = os.path.join(loc, 'Competition_Test_Data.txt')\n",
        "    \n",
        "    print(trainloc)\n",
        "\n",
        "    trainA, trainB, testA, testB = [],[],[],[]\n",
        "    trainS, testU, testS = [],[],[]\n",
        "\n",
        "    f = open(trainloc, 'r')\n",
        "    for line in f:\n",
        "        text = line.strip().split('\\t')\n",
        "        trainA.append(' '.join(word_tokenize(text[2])))\n",
        "        trainB.append(' '.join(word_tokenize(text[3])))\n",
        "        trainS.append(text[0])\n",
        "    f.close()\n",
        "    f = open(testloc, 'r')\n",
        "    for line in f:\n",
        "        text = line.strip().split('\\t')\n",
        "        testA.append(' '.join(word_tokenize(text[1])))\n",
        "        testB.append(' '.join(word_tokenize(text[2])))\n",
        "        #testS.append(text[3])\n",
        "        testU.append(text[0])\n",
        "    f.close()\n",
        "    \n",
        "   \n",
        "    trainS = [int(s) for s in trainS[1:]]\n",
        "    testU = [int(s) for s in testU[1:]]\n",
        "\n",
        "    return [trainA[1:], trainB[1:]], [testA[1:], testB[1:]], [trainS, testS], testU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAhy_ifemh7W",
        "colab_type": "code",
        "outputId": "ec715c17-f30d-4629-bccf-896a2858de04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "traintext, testtext, labels, uid = load_data()\n",
        "fts_train_1, fts_test_1 = generate_sentense_fts(modelname = 'use_v3')\n",
        "fts_train_2, fts_test_2 = generate_sentense_fts(modelname = 'infersent')\n",
        "fts_train_3, fts_test_3 = generate_sentense_fts(modelname = 'infersent2')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./dataset/Competition_Train_Data.txt\n",
            "Preparing data...\n",
            "./dataset/Competition_Train_Data.txt\n",
            "Computing training USE encoding...\n",
            "Preparing data...\n",
            "./dataset/Competition_Train_Data.txt\n",
            "Computing infersentembedding for train data...\n",
            "Nb words kept : 110074/111322 (98.9%)\n",
            "Speed : 929.6 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 109905/111146 (98.9%)\n",
            "Speed : 968.6 sentences/s (gpu mode, bsize=128)\n",
            "Computing infersentembedding for test data...\n",
            "Nb words kept : 27576/27876 (98.9%)\n",
            "Speed : 903.4 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 27606/27912 (98.9%)\n",
            "Speed : 922.0 sentences/s (gpu mode, bsize=128)\n",
            "Preparing data...\n",
            "./dataset/Competition_Train_Data.txt\n",
            "Computing infersent2embedding for train data...\n",
            "Nb words kept : 107610/111322 (96.7%)\n",
            "Speed : 898.3 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 107320/111146 (96.6%)\n",
            "Speed : 906.8 sentences/s (gpu mode, bsize=128)\n",
            "Computing infersent2embedding for test data...\n",
            "Nb words kept : 26846/27876 (96.3%)\n",
            "Speed : 836.8 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 26917/27912 (96.4%)\n",
            "Speed : 858.2 sentences/s (gpu mode, bsize=128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "406qvxVzajdU",
        "colab_type": "code",
        "outputId": "51ec0e1f-ec22-4730-d7e5-bcaed55a0a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainfts, testfts = generate_spacy_cc_similarity()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n",
            "CPU times: user 5min 45s, sys: 2min, total: 7min 45s\n",
            "Wall time: 3min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKkQgtFt-lS7",
        "colab_type": "code",
        "outputId": "f6edf182-6a0e-4625-893b-430a22863ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fts_train_1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "v1AlIW51tAta",
        "colab_type": "code",
        "outputId": "33dd9a89-1e95-4632-87b8-53255c00a159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fts_train_2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 8192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "y8lCSS8VtExL",
        "colab_type": "code",
        "outputId": "56ca57a7-29d3-4f21-ab85-f8fe79405f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fts_train_3.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 8192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "LPFGWgkCFqAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine all the features into a gigantic feature matrix"
      ]
    },
    {
      "metadata": {
        "id": "lBw9e52x3EHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_features =np.c_[fts_train_1,fts_train_2,fts_train_3,feats(traintext[0], traintext[1]),trainfts]\n",
        "test_features = np.c_[fts_test_1,fts_test_2,fts_test_3,feats(testtext[0], testtext[1]),testfts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIohZKLlohH5",
        "colab_type": "code",
        "outputId": "6c19df7b-dac4-48b5-dca8-592b3b47399d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 17415)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "vAtJX40QGEUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticate Colab to use google drive to store the pickle file"
      ]
    },
    {
      "metadata": {
        "id": "Zq4gvl7bI67W",
        "colab_type": "code",
        "outputId": "135397fb-4927-4c56-e49e-8a4b50663971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "371i6I0hK8sH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clO8nQpyGhmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the features and labels into pickle file"
      ]
    },
    {
      "metadata": {
        "id": "98k_D2TMdnYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('final_features.pkl','wb')\n",
        "\n",
        "pickle.dump(train_features, file)\n",
        "pickle.dump(test_features, file)\n",
        "pickle.dump(labels[0], file)\n",
        "pickle.dump(labels[1], file)\n",
        "pickle.dump(uid, file)\n",
        "\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NWoFDAejeBwR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Upload the feature matrix to Google Drive for modelling ~800 MB**"
      ]
    },
    {
      "metadata": {
        "id": "ymbJNG5hMBGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "upload = drive.CreateFile({'title': 'final_features.pkl'})\n",
        "upload.SetContentFile('final_features.pkl')\n",
        "upload.Upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxxSKDq_fMlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Go to Google Drive, select the pkl file Click Get shareable link.\n",
        "Next to \"Anyone with the link,\" click the Down arrow Down.\n",
        "Click More and then On - Public on the web.\n",
        "Click Save.\n",
        "Click Done.\n"
      ]
    },
    {
      "metadata": {
        "id": "Pl_0WohzIFpI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# https://drive.google.com/open?id=1xGT1h3XgWEmZTEkDPMpKIyK7odRNCqy6\n",
        "# https://drive.google.com/file/d/1xGT1h3XgWEmZTEkDPMpKIyK7odRNCqy6/view?usp=sharing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLe1T9j3Nr_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to download again\n",
        "#download = drive.CreateFile({'id': '1PVGVj-BHu5I3ANnEfG4_JKn6RgyzeW3d'})\n",
        "#download.GetContentFile('features.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1AAF9qCFIbQ",
        "colab_type": "code",
        "outputId": "7261e4a0-b25d-4b3c-94ed-a0a43337d52c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2277412\n",
            "-rw-r--r-- 1 root root 1523785255 May 24 11:52 crawl-300d-2M.vec.zip\n",
            "drwxr-xr-x 2 root root       4096 Nov  5 19:58 sample_data\n",
            "-rw-r--r-- 1 root root      29737 Nov  6 18:21 models.py\n",
            "drwxr-xr-x 2 root root       4096 Nov  6 18:22 encoder\n",
            "drwxr-xr-x 2 root root       4096 Nov  6 18:30 __pycache__\n",
            "drwxr-xr-x 4 root root       4096 Nov  6 18:31 dataset\n",
            "-rw-r--r-- 1 root root       2551 Nov  6 18:48 adc.json\n",
            "drwx------ 3 root root       4096 Nov  6 18:49 gdrive\n",
            "-rw-r--r-- 1 root root  808210640 Nov  6 19:20 final_features.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WBPAWuNFgUzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}